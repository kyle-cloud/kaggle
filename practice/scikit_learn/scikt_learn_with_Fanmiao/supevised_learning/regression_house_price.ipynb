{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python",
   "display_name": "Conda"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(506, 13)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "print(boston.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n 'B' 'LSTAT']\n.. _boston_dataset:\n\nBoston house prices dataset\n---------------------------\n\n**Data Set Characteristics:**  \n\n    :Number of Instances: 506 \n\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000's\n\n    :Missing Attribute Values: None\n\n    :Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\nprices and the demand for clean air', J. Environ. Economics & Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.   \n     \n.. topic:: References\n\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n\n"
     ]
    }
   ],
   "source": [
    "print(boston.feature_names)\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "50.0 5.0 22.532806324110677\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.max(boston.target), np.min(boston.target), np.mean(boston.target))"
   ]
  },
  {
   "source": [
    "# preprocess data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.25, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[  0.54011  20.        3.97      0.        0.647     7.203    81.8\n   2.1121    5.      264.       13.      392.8       9.59   ] 33.8\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalerX = StandardScaler().fit(X_train)\n",
    "X_train = scalerX.transform(X_train)\n",
    "X_test = scalerX.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.32076092  0.35553428 -1.00966618 -0.28784917  0.87716097  1.28834383\n  0.4759489  -0.83034371 -0.47659648 -0.81061061 -2.49222645  0.35062335\n -0.39859013] 33.8\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(379, 1)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalery = StandardScaler().fit(y_train)\n",
    "y_train = scalery.transform(y_train)\n",
    "y_test = scalery.transform(y_test)"
   ]
  },
  {
   "source": [
    "# learn data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性模型\n",
    "from sklearn import linear_model\n",
    "\n",
    "clf_sgd = linear_model.SGDRegressor(loss='squared_loss', penalty=None, random_state=42)\n",
    "clf_sgd_12 = linear_model.SGDRegressor(loss='squared_loss', penalty='l2', random_state=42)\n",
    "\n",
    "# svm\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "clf_svr = SVR(kernel='linear')\n",
    "clf_svr_poly = SVR(kernel='poly')\n",
    "clf_svr_rbf = SVR(kernel='rbf')\n",
    "\n",
    "# random forest\n",
    "from sklearn import ensemble\n",
    "\n",
    "clf_et = ensemble.ExtraTreesRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7098518257344566\n",
      "0.7098561728261388\n",
      "0.7017180425033174\n",
      "0.7866677623279259\n",
      "0.8178910469741936\n",
      "0.8842333819989465\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clfs = [clf_sgd, clf_sgd_12, clf_svr, clf_svr_poly, clf_svr_rbf, clf_et]\n",
    "for clf in clfs:\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    print(np.mean(scores))"
   ]
  },
  {
   "source": [
    "# predict data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8064103758449693"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "clf_et.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6636381795948472\n",
      "0.6636463830153829\n",
      "0.650659546421538\n",
      "0.403650651025512\n",
      "0.7559887416340947\n",
      "0.8087278485626916\n"
     ]
    }
   ],
   "source": [
    "for clf in clfs:\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}